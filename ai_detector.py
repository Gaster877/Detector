# -*- coding: utf-8 -*-
"""AI detector.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1arCCE6fSa5jwYv6QlB7EvuWwOICpK_su

# Google Drive (Colab å­˜æª”æ¡ˆéœ€è¦)
"""

from google.colab import drive
drive.mount('/content/drive')

"""#è¨­å®šåˆ†å‰²å‡½æ•¸"""

import os
import random
import shutil

def split_dataset(source_folder, train_folder, val_folder, split_ratio=0.8):
  # è¨­å®šéš¨æ©Ÿç¨®å­ï¼Œç¢ºä¿æ¯æ¬¡çµæœä¸€è‡´
  random.seed(42)

  # å»ºç«‹ç›®æ¨™è³‡æ–™å¤¾
  os.makedirs(train_folder, exist_ok=True)
  os.makedirs(val_folder, exist_ok=True)

  # å–å¾—æ‰€æœ‰åœ–ç‰‡æª”æ¡ˆ
  all_images = [f for f in os.listdir(source_folder) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]

  # æ‰“äº‚é †åº
  random.shuffle(all_images)

  # åˆ†å‰²æ•¸é‡
  split_index = int(len(all_images) * 0.8)
  train_images = all_images[:split_index]
  val_images = all_images[split_index:]

  # è¤‡è£½åœ–ç‰‡åˆ°å°æ‡‰è³‡æ–™å¤¾
  for img in train_images:
      shutil.copy(os.path.join(source_folder, img), os.path.join(train_folder, img))

  for img in val_images:
      shutil.copy(os.path.join(source_folder, img), os.path.join(val_folder, img))

  print(f"å…±è™•ç† {len(all_images)} å¼µåœ–ç‰‡ï¼Œå…¶ä¸­ {len(train_images)} å¼µæ”¾åˆ° trainï¼Œ {len(val_images)} å¼µæ”¾åˆ° valã€‚")

"""##åˆ†å‰²datasetè‡³è¨“ç·´é›†å’Œæ¸¬è©¦é›†(AI dataset)"""

s_folder = '/content/drive/MyDrive/dataset/AI'
t_folder = '/content/data/train/fake'
v_folder = '/content/data/val/fake'
split_dataset(source_folder= s_folder,
              train_folder= t_folder,
              val_folder= v_folder)

"""##åˆ†å‰²datasetè‡³è¨“ç·´é›†å’Œæ¸¬è©¦é›†(Real dataset)"""

s_folder = '/content/drive/MyDrive/dataset/Real'
t_folder = '/content/data/train/real'
v_folder = '/content/data/val/real'
split_dataset(source_folder= s_folder,
              train_folder= t_folder,
              val_folder= v_folder)

"""## æ¸…ç©ºè³‡æ–™å¤¾(If needed)"""

def delete_folder(folder_path):
  # åˆªé™¤è©²è³‡æ–™å¤¾ä¸‹çš„æ‰€æœ‰æª”æ¡ˆèˆ‡å­è³‡æ–™å¤¾
  for filename in os.listdir(folder_path):
      file_path = os.path.join(folder_path, filename)
      if os.path.isfile(file_path) or os.path.islink(file_path):
          os.unlink(file_path)  # åˆªé™¤æª”æ¡ˆæˆ–ç¬¦è™Ÿé€£çµ
      elif os.path.isdir(file_path):
          shutil.rmtree(file_path)  # åˆªé™¤å­è³‡æ–™å¤¾

  print(f"{folder_path} è³‡æ–™å¤¾å…§å®¹å·²æ¸…ç©ºã€‚")

delete_folder('/content/data/train/fake')
delete_folder('/content/data/train/real')
delete_folder('/content/data/val/fake')
delete_folder('/content/data/val/real')

"""#Excution å€åŸŸ"""

# 1. å®‰è£å¿…è¦å¥—ä»¶ï¼ˆColab å¯ç”¨ï¼‰
!pip install torch torchvision matplotlib

# 2. åŒ¯å…¥æ¨¡çµ„
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, models, transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import os
import time

# 3. è¨­å®šè·¯å¾‘èˆ‡åƒæ•¸
data_dir = "/content/data"
batch_size = 32
num_epochs = 6
num_classes = 2  # Real vs AI
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 4. åœ–ç‰‡è½‰æ›èˆ‡è³‡æ–™åŠ è¼‰å™¨
data_transforms = {
    "train": transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
    ]),
    "val": transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
    ]),
}

!find /content/data -type d -name ".ipynb_checkpoints" -exec rm -r {} +
print("âœ… æ¸…é™¤ .ipynb_checkpoints è³‡æ–™å¤¾æˆåŠŸ")

image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])
                  for x in ['train', 'val']}

dataloaders = {x: DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True)
               for x in ['train', 'val']}

# 5. è¼‰å…¥ ResNet æ¨¡å‹ï¼ˆResNet-18ï¼‰
model = models.resnet18(pretrained=True)
model.fc = nn.Linear(model.fc.in_features, num_classes)
model = model.to(device)

# 6. æå¤±èˆ‡å„ªåŒ–å™¨
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.0001)

# 7. è¨“ç·´èˆ‡é©—è­‰å‡½å¼
def train_model(model, dataloaders, criterion, optimizer, num_epochs):
    for epoch in range(num_epochs):
        print(f"\nğŸ” Epoch {epoch+1}/{num_epochs}")
        for phase in ['train', 'val']:
            running_loss = 0.0
            running_corrects = 0
            model.train() if phase == 'train' else model.eval()
            for inputs, labels in dataloaders[phase]:
                inputs = inputs.to(device)
                labels = labels.to(device)

                optimizer.zero_grad()
                with torch.set_grad_enabled(phase == 'train'):
                    outputs = model(inputs)
                    _, preds = torch.max(outputs, 1)
                    loss = criterion(outputs, labels)

                    if phase == 'train':
                        loss.backward()
                        optimizer.step()

                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)

            epoch_loss = running_loss / len(dataloaders[phase].dataset)
            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)
            print(f"{phase.upper()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}")
    return model

# 8. é–‹å§‹è¨“ç·´
trained_model = train_model(model, dataloaders, criterion, optimizer, num_epochs)

# 9. å„²å­˜æ¨¡å‹
torch.save(trained_model.state_dict(), "resnet_ai_detector.pth")
shutil.copy("resnet_ai_detector.pth", "/content/drive/MyDrive/model")
print("âœ… æ¨¡å‹å·²å„²å­˜")

# 10. é æ¸¬å–®å¼µåœ–ç‰‡
from PIL import Image

def predict_image(image_path, model):
    model.eval()
    img = Image.open(image_path).convert("RGB")
    transform = data_transforms['val']
    input_tensor = transform(img).unsqueeze(0).to(device)

    with torch.no_grad():
        output = model(input_tensor)
        _, pred = torch.max(output, 1)
        class_names = image_datasets['train'].classes
        print(f"âœ… é æ¸¬çµæœ: {class_names[pred.item()]}")
        plt.imshow(img)
        plt.title(f"Result: {class_names[pred.item()]}")
        plt.axis('off')
        plt.show()

model = models.resnet18(pretrained=False)
model.fc = nn.Linear(model.fc.in_features, 2)  # å’Œä¹‹å‰ä¿æŒä¸€è‡´
model.load_state_dict(torch.load("resnet_ai_detector.pth", map_location=device))
model = model.to(device)

predict_image('/content/test/test11.png', model)

"""# éŒ¯èª¤æ¨£æœ¬æ”¶é›†"""

# --- è¨­å®šè·¯å¾‘ ---
image_folder = '/content/drive/MyDrive/my_new_dataset/Fake'  # åŸå§‹åœ–ç‰‡è³‡æ–™å¤¾
wrong_folder = '/content/drive/MyDrive/my_new_dataset/wrongly_classified'  # æ”¾éŒ¯èª¤åˆ†é¡åœ–ç‰‡

# --- å‰µå»ºéŒ¯èª¤åˆ†é¡è³‡æ–™å¤¾ ---
os.makedirs(wrong_folder, exist_ok=True)

# --- åœ–åƒé è™•ç†è¦å’Œè¨“ç·´æ™‚ä¸€è‡´ ---
transform = transforms.Compose([
    transforms.Resize((224, 224)),  # æ ¹æ“šä½ çš„æ¨¡å‹è¨­è¨ˆä¿®æ”¹
    transforms.ToTensor(),
])

# --- è¼‰å…¥æ¨¡å‹ ---
model = models.resnet18(pretrained=False)  # æ›¿æ›ç‚ºä½ çš„æ¨¡å‹é¡åˆ¥
model.load_state_dict(torch.load('/content/drive/MyDrive/model/resnet_model.pth'))
model.eval().cuda()  # è‹¥æœ‰ GPU

# --- æ¨è«– ---
with torch.no_grad():
    for filename in os.listdir(image_folder):
        if not filename.lower().endswith(('.jpg', '.png', '.jpeg')): continue

        image_path = os.path.join(image_folder, filename)
        image = Image.open(image_path).convert('RGB')
        input_tensor = transform(image).unsqueeze(0).cuda()

        output = model(input_tensor)
        pred_class = torch.argmax(output, dim=1).item()

        # --- é€™è£¡ä¾ç…§æª”ååˆ¤æ–·çœŸå¯¦æ¨™ç±¤ ---
        if filename.lower().startswith("real"):
            label = 0
        elif filename.lower().startswith("fake"):
            label = 1
        else:
            print(f"ç„¡æ³•åˆ¤æ–·æ¨™ç±¤: {filename}")
            continue

        # --- åˆ¤æ–·éŒ¯èª¤å°±è¤‡è£½åœ–ç‰‡éå» ---
        if pred_class != label:
            shutil.copy(image_path, os.path.join(wrong_folder, filename))
            print(f"[éŒ¯èª¤åˆ†é¡] å·²ç§»å‹•: {filename}")
