{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Google Drive"
      ],
      "metadata": {
        "id": "g7V324O2yjZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "IUvOuZLvyS9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#è¨­å®šåˆ†å‰²å‡½æ•¸"
      ],
      "metadata": {
        "id": "bsl2-jDxkXiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "def split_dataset(source_folder, train_folder, val_folder, split_ratio=0.8):\n",
        "  # è¨­å®šéš¨æ©Ÿç¨®å­ï¼Œç¢ºä¿æ¯æ¬¡çµæœä¸€è‡´\n",
        "  random.seed(42)\n",
        "\n",
        "  # å»ºç«‹ç›®æ¨™è³‡æ–™å¤¾\n",
        "  os.makedirs(train_folder, exist_ok=True)\n",
        "  os.makedirs(val_folder, exist_ok=True)\n",
        "\n",
        "  # å–å¾—æ‰€æœ‰åœ–ç‰‡æª”æ¡ˆ\n",
        "  all_images = [f for f in os.listdir(source_folder) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
        "\n",
        "  # æ‰“äº‚é †åº\n",
        "  random.shuffle(all_images)\n",
        "\n",
        "  # è¨ˆç®—åˆ†å‰²æ•¸é‡\n",
        "  split_index = int(len(all_images) * 0.8)\n",
        "  train_images = all_images[:split_index]\n",
        "  val_images = all_images[split_index:]\n",
        "\n",
        "  # è¤‡è£½åœ–ç‰‡åˆ°å°æ‡‰è³‡æ–™å¤¾\n",
        "  for img in train_images:\n",
        "      shutil.copy(os.path.join(source_folder, img), os.path.join(train_folder, img))\n",
        "\n",
        "  for img in val_images:\n",
        "      shutil.copy(os.path.join(source_folder, img), os.path.join(val_folder, img))\n",
        "\n",
        "  print(f\"å…±è™•ç† {len(all_images)} å¼µåœ–ç‰‡ï¼Œå…¶ä¸­ {len(train_images)} å¼µæ”¾åˆ° trainï¼Œ {len(val_images)} å¼µæ”¾åˆ° valã€‚\")"
      ],
      "metadata": {
        "id": "LNmEGyNripat"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##åˆ†å‰²datasetè‡³è¨“ç·´é›†å’Œæ¸¬è©¦é›†(AI dataset)"
      ],
      "metadata": {
        "id": "PXFhFwYlkeOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s_folder = '/content/drive/MyDrive/dataset/AI'\n",
        "t_folder = '/content/data/train/fake'\n",
        "v_folder = '/content/data/val/fake'\n",
        "split_dataset(source_folder= s_folder,\n",
        "              train_folder= t_folder,\n",
        "              val_folder= v_folder)"
      ],
      "metadata": {
        "id": "ySS8argVj5hn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##åˆ†å‰²datasetè‡³è¨“ç·´é›†å’Œæ¸¬è©¦é›†(Real dataset)"
      ],
      "metadata": {
        "id": "Kef5ZAHUkvF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s_folder = '/content/drive/MyDrive/dataset/Real'\n",
        "t_folder = '/content/data/train/real'\n",
        "v_folder = '/content/data/val/real'\n",
        "split_dataset(source_folder= s_folder,\n",
        "              train_folder= t_folder,\n",
        "              val_folder= v_folder)"
      ],
      "metadata": {
        "id": "zAfx1l_8kK_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## æ¸…ç©ºè³‡æ–™å¤¾(If needed)"
      ],
      "metadata": {
        "id": "nLV2AeLtol5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def delete_folder(folder_path):\n",
        "  # åˆªé™¤è©²è³‡æ–™å¤¾ä¸‹çš„æ‰€æœ‰æª”æ¡ˆèˆ‡å­è³‡æ–™å¤¾\n",
        "  for filename in os.listdir(folder_path):\n",
        "      file_path = os.path.join(folder_path, filename)\n",
        "      if os.path.isfile(file_path) or os.path.islink(file_path):\n",
        "          os.unlink(file_path)  # åˆªé™¤æª”æ¡ˆæˆ–ç¬¦è™Ÿé€£çµ\n",
        "      elif os.path.isdir(file_path):\n",
        "          shutil.rmtree(file_path)  # åˆªé™¤å­è³‡æ–™å¤¾\n",
        "\n",
        "  print(f\"{folder_path} è³‡æ–™å¤¾å…§å®¹å·²æ¸…ç©ºã€‚\")\n",
        "\n",
        "delete_folder('/content/data/train/fake')\n",
        "delete_folder('/content/data/train/real')\n",
        "delete_folder('/content/data/val/fake')\n",
        "delete_folder('/content/data/val/real')"
      ],
      "metadata": {
        "id": "5emMdgFzoCaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Excution"
      ],
      "metadata": {
        "id": "QqcY89m4kydh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ“Œ 1. å®‰è£å¿…è¦å¥—ä»¶ï¼ˆColab å¯ç”¨ï¼‰\n",
        "!pip install torch torchvision matplotlib\n",
        "\n",
        "# ğŸ“Œ 2. åŒ¯å…¥æ¨¡çµ„\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import time\n",
        "\n",
        "# ğŸ“Œ 3. è¨­å®šè·¯å¾‘èˆ‡åƒæ•¸\n",
        "data_dir = \"/content/data\"\n",
        "batch_size = 32\n",
        "num_epochs = 6\n",
        "num_classes = 2  # Real vs AI\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ğŸ“Œ 4. åœ–ç‰‡è½‰æ›èˆ‡è³‡æ–™åŠ è¼‰å™¨\n",
        "data_transforms = {\n",
        "    \"train\": transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "    ]),\n",
        "    \"val\": transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "    ]),\n",
        "}\n",
        "\n",
        "!find /content/data -type d -name \".ipynb_checkpoints\" -exec rm -r {} +\n",
        "print(\"âœ… æ¸…é™¤ .ipynb_checkpoints è³‡æ–™å¤¾æˆåŠŸ\")\n",
        "\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n",
        "                  for x in ['train', 'val']}\n",
        "\n",
        "dataloaders = {x: DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True)\n",
        "               for x in ['train', 'val']}\n",
        "\n",
        "# ğŸ“Œ 5. è¼‰å…¥ ResNet æ¨¡å‹ï¼ˆResNet-18ï¼‰\n",
        "model = models.resnet18(pretrained=True)\n",
        "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "model = model.to(device)\n",
        "\n",
        "# ğŸ“Œ 6. æå¤±èˆ‡å„ªåŒ–å™¨\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "# ğŸ“Œ 7. è¨“ç·´èˆ‡é©—è­‰å‡½å¼\n",
        "def train_model(model, dataloaders, criterion, optimizer, num_epochs):\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nğŸ” Epoch {epoch+1}/{num_epochs}\")\n",
        "        for phase in ['train', 'val']:\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "            model.train() if phase == 'train' else model.eval()\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "            print(f\"{phase.upper()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
        "    return model\n",
        "\n",
        "# ğŸ“Œ 8. é–‹å§‹è¨“ç·´\n",
        "trained_model = train_model(model, dataloaders, criterion, optimizer, num_epochs)"
      ],
      "metadata": {
        "id": "XeScBoZFXmUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ“Œ 9. å„²å­˜æ¨¡å‹\n",
        "torch.save(trained_model.state_dict(), \"resnet_ai_detector.pth\")\n",
        "shutil.copy(\"resnet_ai_detector.pth\", \"/content/drive/MyDrive/model\")\n",
        "print(\"âœ… æ¨¡å‹å·²å„²å­˜\")\n",
        "\n",
        "# ğŸ“Œ 10. é æ¸¬å–®å¼µåœ–ç‰‡\n",
        "from PIL import Image\n",
        "\n",
        "def predict_image(image_path, model):\n",
        "    model.eval()\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "    transform = data_transforms['val']\n",
        "    input_tensor = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "        _, pred = torch.max(output, 1)\n",
        "        class_names = image_datasets['train'].classes\n",
        "        print(f\"âœ… é æ¸¬çµæœ: {class_names[pred.item()]}\")\n",
        "        plt.imshow(img)\n",
        "        plt.title(f\"Result: {class_names[pred.item()]}\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "g-zTW-UxO54M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.resnet18(pretrained=False)\n",
        "model.fc = nn.Linear(model.fc.in_features, 2)  # å’Œä¹‹å‰ä¿æŒä¸€è‡´\n",
        "model.load_state_dict(torch.load(\"resnet_ai_detector.pth\", map_location=device))\n",
        "model = model.to(device)\n",
        "\n",
        "predict_image('/content/test/test11.png', model)"
      ],
      "metadata": {
        "id": "QyPOJynKxKNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# éŒ¯èª¤æ¨£æœ¬æ”¶é›†"
      ],
      "metadata": {
        "id": "XcXO6ZagX7gQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- è¨­å®šè·¯å¾‘ ---\n",
        "image_folder = '/content/drive/MyDrive/my_new_dataset/Fake'  # åŸå§‹åœ–ç‰‡è³‡æ–™å¤¾\n",
        "wrong_folder = '/content/drive/MyDrive/my_new_dataset/wrongly_classified'  # æ”¾éŒ¯èª¤åˆ†é¡åœ–ç‰‡\n",
        "\n",
        "# --- å‰µå»ºéŒ¯èª¤åˆ†é¡è³‡æ–™å¤¾ ---\n",
        "os.makedirs(wrong_folder, exist_ok=True)\n",
        "\n",
        "# --- åœ–åƒé è™•ç†è¦å’Œè¨“ç·´æ™‚ä¸€è‡´ ---\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # æ ¹æ“šä½ çš„æ¨¡å‹è¨­è¨ˆä¿®æ”¹\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# --- è¼‰å…¥æ¨¡å‹ ---\n",
        "model = models.resnet18(pretrained=False)  # æ›¿æ›ç‚ºä½ çš„æ¨¡å‹é¡åˆ¥\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/model/resnet_model.pth'))\n",
        "model.eval().cuda()  # è‹¥æœ‰ GPU\n",
        "\n",
        "# --- æ¨è«– ---\n",
        "with torch.no_grad():\n",
        "    for filename in os.listdir(image_folder):\n",
        "        if not filename.lower().endswith(('.jpg', '.png', '.jpeg')): continue\n",
        "\n",
        "        image_path = os.path.join(image_folder, filename)\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        input_tensor = transform(image).unsqueeze(0).cuda()\n",
        "\n",
        "        output = model(input_tensor)\n",
        "        pred_class = torch.argmax(output, dim=1).item()\n",
        "\n",
        "        # --- é€™è£¡ä¾ç…§æª”ååˆ¤æ–·çœŸå¯¦æ¨™ç±¤ ---\n",
        "        if filename.lower().startswith(\"real\"):\n",
        "            label = 0\n",
        "        elif filename.lower().startswith(\"fake\"):\n",
        "            label = 1\n",
        "        else:\n",
        "            print(f\"ç„¡æ³•åˆ¤æ–·æ¨™ç±¤: {filename}\")\n",
        "            continue\n",
        "\n",
        "        # --- åˆ¤æ–·éŒ¯èª¤å°±è¤‡è£½åœ–ç‰‡éå» ---\n",
        "        if pred_class != label:\n",
        "            shutil.copy(image_path, os.path.join(wrong_folder, filename))\n",
        "            print(f\"[éŒ¯èª¤åˆ†é¡] å·²ç§»å‹•: {filename}\")\n"
      ],
      "metadata": {
        "id": "SWkVFLxxUwgS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}